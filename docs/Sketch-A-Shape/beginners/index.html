<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Sketch-A-Shape/beginners" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.6.3">
<title data-rh="true">For Beginners | My Site</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://rhy2781.github.io/2d-Converter-Docs/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://rhy2781.github.io/2d-Converter-Docs/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://rhy2781.github.io/2d-Converter-Docs/docs/Sketch-A-Shape/beginners"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="For Beginners | My Site"><meta data-rh="true" name="description" content="Welcome to the Sketch-A-Shape Machine Learning Documentation! This page is designed for those who want to gain a high-level understanding of how the machine learning model behind Sketch-A-Shape works, without requiring any prior background knowledge."><meta data-rh="true" property="og:description" content="Welcome to the Sketch-A-Shape Machine Learning Documentation! This page is designed for those who want to gain a high-level understanding of how the machine learning model behind Sketch-A-Shape works, without requiring any prior background knowledge."><link data-rh="true" rel="icon" href="/2d-Converter-Docs/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://rhy2781.github.io/2d-Converter-Docs/docs/Sketch-A-Shape/beginners"><link data-rh="true" rel="alternate" href="https://rhy2781.github.io/2d-Converter-Docs/docs/Sketch-A-Shape/beginners" hreflang="en"><link data-rh="true" rel="alternate" href="https://rhy2781.github.io/2d-Converter-Docs/docs/Sketch-A-Shape/beginners" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/2d-Converter-Docs/blog/rss.xml" title="My Site RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/2d-Converter-Docs/blog/atom.xml" title="My Site Atom Feed"><link rel="stylesheet" href="/2d-Converter-Docs/assets/css/styles.2e27b0d1.css">
<script src="/2d-Converter-Docs/assets/js/runtime~main.4453e415.js" defer="defer"></script>
<script src="/2d-Converter-Docs/assets/js/main.27408687.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/2d-Converter-Docs/"><div class="navbar__logo"><img src="/2d-Converter-Docs/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/2d-Converter-Docs/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">2D Converter</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/2d-Converter-Docs/docs/Workflow/palceholder">Docs</a><a class="navbar__item navbar__link" href="/2d-Converter-Docs/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/GlennT18/2D-Converter" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/2d-Converter-Docs/docs/Workflow/palceholder">Workflow</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/2d-Converter-Docs/docs/Pixel2Mesh/summary">Pixel2Mesh</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/2d-Converter-Docs/docs/Sketch-A-Shape/beginners">Sketch-A-Shape</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/2d-Converter-Docs/docs/Sketch-A-Shape/beginners">For Beginners</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/2d-Converter-Docs/docs/Sketch-A-Shape/introduction">Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/2d-Converter-Docs/docs/Sketch-A-Shape/stage_1">Stage I Auto-Encoder</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/2d-Converter-Docs/docs/Sketch-A-Shape/stage_2">Stage II - Prior Model</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/2d-Converter-Docs/docs/intro">temporary placeholder?</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/2d-Converter-Docs/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Sketch-A-Shape</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">For Beginners</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>For Beginners</h1></header>
<p>Welcome to the <strong>Sketch-A-Shape Machine Learning Documentation</strong>! This page is designed for those who want to gain a high-level understanding of how the machine learning model behind Sketch-A-Shape works, without requiring any prior background knowledge.</p>
<p>If you&#x27;re looking for more <strong>in-depth technical details</strong>, we recommend starting with the <a href="/2d-Converter-Docs/docs/Sketch-A-Shape/introduction"><strong>Introduction Page</strong></a> and then proceeding to the <a href="/2d-Converter-Docs/docs/Sketch-A-Shape/stage_1"><strong>Stage I Auto-Encoder</strong></a> and <a href="/2d-Converter-Docs/docs/Sketch-A-Shape/stage_2"><strong>Stage II - Prior Model</strong></a> pages, where you&#x27;ll find comprehensive explanations and step-by-step breakdowns of the system&#x27;s development and implementation.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="objective--overview">Objective + Overview<a href="#objective--overview" class="hash-link" aria-label="Direct link to Objective + Overview" title="Direct link to Objective + Overview">​</a></h2>
<p>Create <strong>3D shapes</strong> from sketches of varying complexity using <strong>only computer-generated images</strong>, without requiring a <strong>paired dataset of sketch/shape</strong>—unlike approaches such as <strong>Pixel Mesh</strong>.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="training">Training<a href="#training" class="hash-link" aria-label="Direct link to Training" title="Direct link to Training">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="stage-1">Stage 1<a href="#stage-1" class="hash-link" aria-label="Direct link to Stage 1" title="Direct link to Stage 1">​</a></h3>
<ul>
<li>Shapes are converted into a <strong>sequence of symbols</strong> (<em>shape embeddings, Z</em>) using an <strong>autoencoder</strong>.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="stage-2">Stage 2<a href="#stage-2" class="hash-link" aria-label="Direct link to Stage 2" title="Direct link to Stage 2">​</a></h3>
<ul>
<li>A <strong>transformer model</strong> learns to predict these symbols based on <strong>features extracted from rendered images</strong>.</li>
<li>These features are derived from a <strong>pre-trained model</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="shape-generation">Shape Generation<a href="#shape-generation" class="hash-link" aria-label="Direct link to Shape Generation" title="Direct link to Shape Generation">​</a></h2>
<ul>
<li>During shape generation, the <strong>transformer model</strong> uses the sketch to <strong>iteratively predict the shape embedding</strong> (<em>compact numerical representations of 3D shapes</em>—think of shapes represented as numerical data).</li>
<li>Finally, the <strong>autoencoder’s decoder</strong> converts these embeddings into a <strong>3D shape</strong>.</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="training-discrete-autoencoder">Training Discrete Autoencoder<a href="#training-discrete-autoencoder" class="hash-link" aria-label="Direct link to Training Discrete Autoencoder" title="Direct link to Training Discrete Autoencoder">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="refer-back-to-stage-1">Refer back to Stage 1<a href="#refer-back-to-stage-1" class="hash-link" aria-label="Direct link to Refer back to Stage 1" title="Direct link to Refer back to Stage 1">​</a></h3>
<p>In Stage 1, the <strong>autoencoder</strong> is used to compress the 3D shapes into a sequence of meaningful numerical data representation called the <strong>“shape embedding”</strong>. To do this, a specific type of autoencoder called <strong>VQ-VAE (Vector Quantized Variational Auto-Encoder)</strong> is used.</p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-an-autoencoder">What is an Autoencoder?<a href="#what-is-an-autoencoder" class="hash-link" aria-label="Direct link to What is an Autoencoder?" title="Direct link to What is an Autoencoder?">​</a></h3>
<p>In simple terms, an <strong>autoencoder</strong> is a type of machine learning model that learns to compress data into a smaller representation and then reconstruct it back to its original form. It works by first encoding the data into a compact, lower-dimensional format and then decoding it to recover the original data as closely as possible. Since autoencoders typically work with <strong>unlabeled data</strong>, they are considered an <strong>unsupervised learning model</strong>.</p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-does-that-make-sense">How does that make sense?<a href="#how-does-that-make-sense" class="hash-link" aria-label="Direct link to How does that make sense?" title="Direct link to How does that make sense?">​</a></h3>
<p>By compressing information, it forces the model to learn to extract the most important features from the input. The autoencoder at the start may be bad at reconstruction, but with training, it minimizes the <strong>“reconstruction loss”</strong> by adjusting internal parameters.</p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-can-it-train-on-data-and-improve-without-adding-more-data">How can it train on data and improve without adding more data?<a href="#how-can-it-train-on-data-and-improve-without-adding-more-data" class="hash-link" aria-label="Direct link to How can it train on data and improve without adding more data?" title="Direct link to How can it train on data and improve without adding more data?">​</a></h3>
<p>Even with the same data, it’s possible to improve because it <strong>iteratively learns with epochs</strong> (a single pass through the training dataset). During each epoch, the autoencoder processes the data, adjusts with the loss (reconstruction error), and backpropagates the information to adjust weights using an optimizer. With more epochs, the model gets fine-tuned, although after a certain number of epochs, the adjustment in loss becomes minimal, making further adjustments not worth the time.</p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="this-still-doesnt-make-sense-could-you-give-an-example">This still doesn&#x27;t make sense, could you give an example?<a href="#this-still-doesnt-make-sense-could-you-give-an-example" class="hash-link" aria-label="Direct link to This still doesn&#x27;t make sense, could you give an example?" title="Direct link to This still doesn&#x27;t make sense, could you give an example?">​</a></h3>
<p>To generalize things, <strong>autoencoders</strong> are really good at learning how to <strong>“reconstruct something”</strong>.</p>
<p>Think of it this way: You are given a <strong>Lego car</strong> and told to learn how to build it. With no instructions, you would learn by taking it apart and putting it together a bunch of times. At first, you might be slow and make mistakes, but eventually, you would get really good at building this Lego car with high accuracy. This is essentially the <strong>decode/encode</strong> aspect of the autoencoder.</p>
<p>Now, someone comes along and hands you the Lego pieces for a <strong>different Lego car</strong> and tells you to build the car! You don’t have any instructions again, but since you learned how to put together a Lego car in the past, you are able to apply that knowledge and build the new car! It might not be a perfect result, but you would most likely be able to create something resembling a car with those pieces!</p>
<p>Now, imagine you learned how to reconstruct <strong>a lot of different objects</strong> instead of just a car—you could basically make anything you were asked to make! This is what we are aiming for when we provide a <strong>wide variety of training data</strong> to the model.</p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="so-about-that-vq-vae">So about that VQ-VAE?<a href="#so-about-that-vq-vae" class="hash-link" aria-label="Direct link to So about that VQ-VAE?" title="Direct link to So about that VQ-VAE?">​</a></h3>
<p><strong>Vector Quantized Variational Autoencoders</strong> (VQ-VAE) bring the idea of object reconstruction to the next level. Instead of just learning how to build and rebuild the Lego car directly, you are given a special instructions book with a finite set of <strong>building patterns</strong> (like how to make wheels, doors, engines). Each of these patterns is assigned a unique code in the instruction book. When you see a new Lego creation, instead of memorizing each single piece’s position, you translate it into those pre-learned patterns and their corresponding codes.</p>
<p>This is essentially what the <strong>VQ-VAE</strong> does: it compresses complex data (like a 3D shape) into a <strong>discrete set of symbols</strong> from a <strong>predefined dictionary</strong>.</p>
<p>When it’s time to reconstruct the object, you use these codes from the instructions book to recreate the original structure. This approach makes the model more efficient and scalable as it focuses on reusing a set of meaningful <strong>“building blocks”</strong> rather than trying to understand and memorize every tiny detail.</p>
<p>Overall, <strong>VQ-VAE</strong> breaks down data into <strong>reusable pieces</strong>, allowing for better reconstruction quality and a more interpretable representation!</p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="now-going-back-to-how-we-train-it">Now going back to how we train it:<a href="#now-going-back-to-how-we-train-it" class="hash-link" aria-label="Direct link to Now going back to how we train it:" title="Direct link to Now going back to how we train it:">​</a></h3>
<p>Now that we have established that we are using a <strong>VQ-VAE</strong>, to train the model we first need a <strong>3D shape (S)</strong> and an <strong>encoder (E)</strong> which converts it into a sequence of <strong>discrete indices (Z)</strong>.</p>
<ul>
<li>Formula:<br>
<code>Z = VQ(E(S))</code></li>
</ul>
<p>A <strong>decoder (D)</strong> then reconstructs the shape (<strong>S’</strong>) from these indices.</p>
<ul>
<li>Formula:<br>
<code>S’ = D(Z)</code></li>
</ul>
<p>A <strong>reconstruction loss (Lrec)</strong> is generated to ensure the reconstructed shape matches the original.</p>
<p>A <strong>commitment loss</strong> encourages the encoder to stick to a specific dictionary entry.</p>
<p>An <strong>exponential moving average</strong> calculation ensures the dictionary entries gradually align with the encoder&#x27;s outputs.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="stage-2-masked-transformer">(Stage 2) Masked Transformer<a href="#stage-2-masked-transformer" class="hash-link" aria-label="Direct link to (Stage 2) Masked Transformer" title="Direct link to (Stage 2) Masked Transformer">​</a></h2>
<p>The goal of <strong>Stage 2</strong> is to train a model that can generate <strong>shape indices (Z)</strong> from a sketch during inference. To achieve this, we use a <strong>bi-directional transformer network</strong> that takes two main inputs:</p>
<ul>
<li><strong>Shape Indices (Z):</strong> from Stage 1</li>
<li><strong>Features from 3D Renderings (C):</strong> Extracted from images of the shapes using a pre-trained model</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="masking-indices">Masking Indices<a href="#masking-indices" class="hash-link" aria-label="Direct link to Masking Indices" title="Direct link to Masking Indices">​</a></h3>
<p>During training, we <strong>randomly mask</strong> some shape indices (<strong>Z</strong>) with a special placeholder token, creating a masked version of <strong>Z_msk</strong>. The model’s task is to predict the missing indices using the available information from the sketch features (<strong>C</strong>).</p>
<p>The objective here is for the model to learn to reconstruct the original indices (<strong>Z</strong>) from the masked version while relying on the sketch features from (<strong>C</strong>). The <strong>loss function</strong> measures how accurately the model predicts these missing indices.</p>
<p>Additionally, at each training step, we randomly pick an <strong>image view</strong> of the 3D shape. This image is passed through a pre-trained model to extract features (meaningful details), and once these features are extracted, they are converted into a sequence using a simple mapping network (MLP layers).</p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="in-short">In Short<a href="#in-short" class="hash-link" aria-label="Direct link to In Short" title="Direct link to In Short">​</a></h3>
<p><strong>Stage 2</strong> trains a <strong>transformer</strong> to predict shape indices from sketches by leveraging powerful features from pre-trained models, then fine-tuning them with <strong>cross attention</strong> and <strong>masking strategies</strong>.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="inference">Inference<a href="#inference" class="hash-link" aria-label="Direct link to Inference" title="Direct link to Inference">​</a></h2>
<p>Inference is the <strong>generation phase</strong>, where the sketch is first converted into a sequence of <strong>local features</strong> using the pre-trained model. These features act as guiding information for the transformer model.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="iterative-decoding">Iterative Decoding<a href="#iterative-decoding" class="hash-link" aria-label="Direct link to Iterative Decoding" title="Direct link to Iterative Decoding">​</a></h3>
<p>Iterative decoding starts with a <strong>fully masked</strong> set of indices. At each step, the transformer predicts the unmasked shape sequence using the sketch features. A fraction of the most confident predictions are accepted and remain unmasked, while the rest are reset for the next iteration.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="classifier-free-guidance">Classifier-Free Guidance<a href="#classifier-free-guidance" class="hash-link" aria-label="Direct link to Classifier-Free Guidance" title="Direct link to Classifier-Free Guidance">​</a></h3>
<p>This is also applied to refine predictors. This eventually leads to a final reconstruction of every token being unmasked, and a final sequence of tokens is passed through the <strong>shape decoder</strong> (from Stage 1), which reveals the final 3D object.</p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="final-note">Final Note<a href="#final-note" class="hash-link" aria-label="Direct link to Final Note" title="Direct link to Final Note">​</a></h3>
<p>Ultimately, this process can be done multiple times with the same sketch to produce <strong>different 3D shapes</strong>. This ties into the concept of <strong>zero-shot learning</strong> (although the term was unused earlier, the concept should feel familiar!), where the model is able to generate new 3D shapes without having seen specific examples of them during training.</p>
<p>To connect this back to the Lego analogy: imagine you’ve learned how to build a Lego car using a set of instructions. Now, someone gives you a new set of Lego pieces, and you are able to apply what you learned about building a car and create something that resembles a car—<strong>even though you&#x27;ve never seen this specific Lego car before</strong>. Similarly, the model can generate new 3D shapes based on the general understanding it gained from the training data, without having seen exact examples of the shapes it’s being asked to generate.</p>
<p>By leveraging the pre-trained features and the learned embeddings from the <strong>autoencoder</strong> and <strong>transformer model</strong>, the system doesn’t require paired training data (like specific sketches matched with 3D shapes). Instead, it generalizes the learned representations to infer new shapes from previously unseen sketches—effectively working in a zero-shot setting. This allows the model to create a diverse set of 3D shapes based on the same input, making it highly versatile and capable of creative shape generation even with limited or no prior examples of the exact object being sketched.</p>
<p>Thus, the combination of the <strong>VQ-VAE autoencoder</strong> and the <strong>masked transformer model</strong> enables the system to handle not only <strong>in-domain</strong> data and enables flexible and scalable 3D shape generation.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Sketch-A-Shape/beginners.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/2d-Converter-Docs/docs/Pixel2Mesh/summary"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Introduction</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/2d-Converter-Docs/docs/Sketch-A-Shape/introduction"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Introduction</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#objective--overview" class="table-of-contents__link toc-highlight">Objective + Overview</a></li><li><a href="#training" class="table-of-contents__link toc-highlight">Training</a><ul><li><a href="#stage-1" class="table-of-contents__link toc-highlight">Stage 1</a></li><li><a href="#stage-2" class="table-of-contents__link toc-highlight">Stage 2</a></li></ul></li><li><a href="#shape-generation" class="table-of-contents__link toc-highlight">Shape Generation</a></li><li><a href="#training-discrete-autoencoder" class="table-of-contents__link toc-highlight">Training Discrete Autoencoder</a><ul><li><a href="#refer-back-to-stage-1" class="table-of-contents__link toc-highlight">Refer back to Stage 1</a></li><li><a href="#what-is-an-autoencoder" class="table-of-contents__link toc-highlight">What is an Autoencoder?</a></li><li><a href="#how-does-that-make-sense" class="table-of-contents__link toc-highlight">How does that make sense?</a></li><li><a href="#how-can-it-train-on-data-and-improve-without-adding-more-data" class="table-of-contents__link toc-highlight">How can it train on data and improve without adding more data?</a></li><li><a href="#this-still-doesnt-make-sense-could-you-give-an-example" class="table-of-contents__link toc-highlight">This still doesn&#39;t make sense, could you give an example?</a></li><li><a href="#so-about-that-vq-vae" class="table-of-contents__link toc-highlight">So about that VQ-VAE?</a></li><li><a href="#now-going-back-to-how-we-train-it" class="table-of-contents__link toc-highlight">Now going back to how we train it:</a></li></ul></li><li><a href="#stage-2-masked-transformer" class="table-of-contents__link toc-highlight">(Stage 2) Masked Transformer</a><ul><li><a href="#masking-indices" class="table-of-contents__link toc-highlight">Masking Indices</a></li><li><a href="#in-short" class="table-of-contents__link toc-highlight">In Short</a></li></ul></li><li><a href="#inference" class="table-of-contents__link toc-highlight">Inference</a><ul><li><a href="#iterative-decoding" class="table-of-contents__link toc-highlight">Iterative Decoding</a></li><li><a href="#classifier-free-guidance" class="table-of-contents__link toc-highlight">Classifier-Free Guidance</a></li><li><a href="#final-note" class="table-of-contents__link toc-highlight">Final Note</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/2d-Converter-Docs/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://x.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/2d-Converter-Docs/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>