"use strict";(self.webpackChunk_2_d_converter_docs=self.webpackChunk_2_d_converter_docs||[]).push([[8425],{707:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>l,frontMatter:()=>s,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"Sketch-A-Shape/stage_1","title":"Stage I Auto-Encoder","description":"For Stage I, we train our own auto-encoder that is capable of compressing/decompressing 3d objects.","source":"@site/docs/Sketch-A-Shape/stage_1.md","sourceDirName":"Sketch-A-Shape","slug":"/Sketch-A-Shape/stage_1","permalink":"/2d-Converter-Docs/docs/Sketch-A-Shape/stage_1","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Sketch-A-Shape/stage_1.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docSidebar","previous":{"title":"Introduction","permalink":"/2d-Converter-Docs/docs/Sketch-A-Shape/introduction"},"next":{"title":"Stage II - Prior Model","permalink":"/2d-Converter-Docs/docs/Sketch-A-Shape/stage_2"}}');var o=n(4848),a=n(8453);const s={},i="Stage I Auto-Encoder",c={},d=[{value:"Dataset",id:"dataset",level:2},{value:"Binary Voxel Representation",id:"binary-voxel-representation",level:3},{value:"Auto-Encoder Structure",id:"auto-encoder-structure",level:2},{value:"Encoder",id:"encoder",level:3},{value:"Vector Quantized",id:"vector-quantized",level:3},{value:"Decoder",id:"decoder",level:3}];function h(e){const t={a:"a",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"stage-i-auto-encoder",children:"Stage I Auto-Encoder"})}),"\n",(0,o.jsx)(t.p,{children:"For Stage I, we train our own auto-encoder that is capable of compressing/decompressing 3d objects."}),"\n",(0,o.jsx)(t.h2,{id:"dataset",children:"Dataset"}),"\n",(0,o.jsxs)(t.p,{children:["For this model, we leverage the ShapeNet data set that is available on hugging faces. The information for this can be\nfound ",(0,o.jsx)(t.a,{href:"https://shapenet.org",children:"here"}),". Anyone can create an account and request access to this dataset."]}),"\n",(0,o.jsx)(t.h3,{id:"binary-voxel-representation",children:"Binary Voxel Representation"}),"\n",(0,o.jsxs)(t.p,{children:["While there are multiple forms for this 3d shape data, we leverage the binary voxel representation for simplicity. This\nformat is stored in ",(0,o.jsx)(t.a,{href:"https://www.patrickmin.com/binvox/",children:".binvox files"}),". A python script to read and write .binvox files\nis available on on ",(0,o.jsx)(t.a,{href:"https://github.com/dimatura/binvox-rw-py/blob/public/binvox_rw.py",children:"Github"})]}),"\n",(0,o.jsx)(t.h2,{id:"auto-encoder-structure",children:"Auto-Encoder Structure"}),"\n",(0,o.jsxs)(t.p,{children:["The auto encoder implements the VQVAE architecture that is presented in this\n",(0,o.jsx)(t.a,{href:"https://arxiv.org/abs/1711.00937",children:"Research Paper"})," We go on to also take into account the implementation details\npresented in Section C of the ",(0,o.jsx)(t.a,{href:"https://arxiv.org/abs/2307.03869",children:"Original Paper"}),".\nA simple pytorch implementation was adopted from this ",(0,o.jsx)(t.a,{href:"https://github.com/airalcorn2/vqvae-pytorch",children:"Github"})]}),"\n",(0,o.jsx)(t.h3,{id:"encoder",children:"Encoder"}),"\n",(0,o.jsx)(t.p,{children:"For the encoder, we take our original shape size of 128 x 128 x 128 and reduce this down to 8 x 8 x 8. This additionally\nproduces 64 output channels that we use for the vector quantized portion of the auto-encoder"}),"\n",(0,o.jsx)(t.h3,{id:"vector-quantized",children:"Vector Quantized"}),"\n",(0,o.jsx)(t.p,{children:"We then quantize the result of our encoding process in order to guide the shape embeddings towards a shape that is\nplausible given all the data that our model has seen."}),"\n",(0,o.jsx)(t.h3,{id:"decoder",children:"Decoder"}),"\n",(0,o.jsx)(t.p,{children:"For the decoder section, we take the quantized vector and we upscale the size to a 32 x 32 x 32 grid size and an output\nchannel of 1 which denotes whether or not the voxel block is present in the decompressed image"})]})}function l(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>i});var r=n(6540);const o={},a=r.createContext(o);function s(e){const t=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),r.createElement(a.Provider,{value:t},e.children)}}}]);