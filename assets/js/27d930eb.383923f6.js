"use strict";(self.webpackChunk_2_d_converter_docs=self.webpackChunk_2_d_converter_docs||[]).push([[3595],{4534:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>h,default:()=>d,frontMatter:()=>o,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"Pixel2Mesh/summary","title":"Introduction","description":"Training Process Overview","source":"@site/docs/Pixel2Mesh/summary.md","sourceDirName":"Pixel2Mesh","slug":"/Pixel2Mesh/summary","permalink":"/2d-Converter-Docs/docs/Pixel2Mesh/summary","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Pixel2Mesh/summary.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docSidebar","previous":{"title":"this is the workflow","permalink":"/2d-Converter-Docs/docs/Workflow/palceholder"},"next":{"title":"For Beginners","permalink":"/2d-Converter-Docs/docs/Sketch-A-Shape/beginners"}}');var r=s(4848),i=s(8453);const o={},h="Introduction",a={},l=[{value:"Overview",id:"overview",level:2},{value:"Key Components",id:"key-components",level:2},{value:"Image Encoder",id:"image-encoder",level:3},{value:"Mesh Decoder",id:"mesh-decoder",level:3},{value:"Template Mesh",id:"template-mesh",level:3},{value:"Loss Function",id:"loss-function",level:3},{value:"Loss Functions Used to Train the Model and Guide Mesh Generation:",id:"loss-functions-used-to-train-the-model-and-guide-mesh-generation",level:4}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"introduction",children:"Introduction"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Training Process Overview",src:s(3448).A+"",width:"1212",height:"307"})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsxs)(n.p,{children:["Pixel2Mesh employs a deep learning approach centered around ",(0,r.jsx)(n.strong,{children:"GCNs"})," (Graph Convolutional Networks). The process begins with a ",(0,r.jsx)(n.strong,{children:"convolutional neural network"})," extracting rich visual features from the input image. These features, along with the initial ",(0,r.jsx)(n.strong,{children:"template mesh"}),", are then fed into a series of GCN layers, which slowly deform the template mesh to match the shape within the image. ",(0,r.jsx)(n.strong,{children:"Mesh pooling layers"})," help to downsample the mesh throughout the process, enabling a coarse-to-fine approach (look at the image above)."]}),"\n",(0,r.jsxs)(n.p,{children:["The model is trained by comparing the predicted 3D mesh to a ",(0,r.jsx)(n.strong,{children:"ground truth mesh"})," (hence the paired dataset), using a ",(0,r.jsx)(n.strong,{children:"loss function"})," that incorporates:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Chamfer distance"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Edge length loss"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Normal loss"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Deformation"})," and ",(0,r.jsx)(n.strong,{children:"refinement"})]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"key-components",children:"Key Components"}),"\n",(0,r.jsx)(n.h3,{id:"image-encoder",children:"Image Encoder"}),"\n",(0,r.jsx)(n.p,{children:"The image encoder takes in the input 2D image and extracts a rich set of features that capture important visual information about the object's shape, appearance, and context."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Implementation"}),": Uses a CNN like ResNet or VGG. In PyTorch, this involves using ",(0,r.jsx)(n.code,{children:"Conv2D"}),", ",(0,r.jsx)(n.code,{children:"MaxPool2d"}),", and ",(0,r.jsx)(n.code,{children:"BatchNorm2d"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Output"}),": Feature map\u2014a multichannel representation of the image."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"mesh-decoder",children:"Mesh Decoder"}),"\n",(0,r.jsxs)(n.p,{children:["This is the GCN part of the model. It takes an image feature map from the image encoder and an initial ",(0,r.jsx)(n.strong,{children:"template mesh"})," (e.g., a sphere,ellipsoid or icosphere) as inputs and iteratively deforms the template mesh to match the shape represented in the image. (refer to bottom right of image at top of page)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Structure"}),": A series of GCN layers, each followed by a ",(0,r.jsx)(n.strong,{children:"mesh pooling layer"})," to perform downsampling of the mesh. In PyTorch, ",(0,r.jsx)(n.code,{children:"torch.nn.Module"})," is used to perform message passing/vertex updates based on neighbors."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"template-mesh",children:"Template Mesh"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"template mesh"})," is a predefined 3D mesh that serves as the starting point. It is stored as tensors in PyTorch, and operations on it are performed. Feature extraction works with ",(0,r.jsx)(n.code,{children:"torch.nn.Linear"}),", ",(0,r.jsx)(n.code,{children:"torch.nn.ReLU"}),", and ",(0,r.jsx)(n.code,{children:"torch.nn.BatchNorm1d"}),"."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"loss-function",children:"Loss Function"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"loss function"})," measures how well the predicted mesh matches the ground truth 3D shape. It provides a learning signal that guides the model's training. PyTorch functionality is used to calculate these loss functions."]}),"\n",(0,r.jsx)(n.h4,{id:"loss-functions-used-to-train-the-model-and-guide-mesh-generation",children:"Loss Functions Used to Train the Model and Guide Mesh Generation:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Chamfer"}),": Measures the distance between each vertex in the predicted mesh and the closest vertex in the ground truth mesh, and vice versa."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Normal"}),": This advanced loss enforces consistency between the predicted and ground truth mesh."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Laplacian"}),": Prevents vertices from moving too freely, avoiding deformations or intersections in the mesh. Helps with smoothness by calculating how much vertices move relative to their neighbors using Laplacian coordinates."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Edge"}),": Prevents vertices from flying away or creating overly long edges in the mesh. It penalizes any large changes in edge length to ensure the mesh is tightly connected."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:["For more details, visit the ",(0,r.jsx)(n.a,{href:"https://github.com/Wapity/Pixel2Mesh-Pytorch/tree/master/tensor",children:"Pixel2Mesh-Pytorch GitHub repository"}),". This PyTorch implementation is based on the original Pixel2Mesh implementation."]})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},3448:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/p2m-329a769f101c00b63a4e532d05ff119e.png"},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>h});var t=s(6540);const r={},i=t.createContext(r);function o(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function h(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);