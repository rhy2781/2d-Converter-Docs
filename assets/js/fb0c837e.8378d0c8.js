"use strict";(self.webpackChunk_2_d_converter_docs=self.webpackChunk_2_d_converter_docs||[]).push([[7759],{5373:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>d,contentTitle:()=>s,default:()=>p,frontMatter:()=>o,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"Sketch-A-Shape/introduction","title":"Introduction","description":"[arXiv]","source":"@site/docs/Sketch-A-Shape/introduction.md","sourceDirName":"Sketch-A-Shape","slug":"/Sketch-A-Shape/introduction","permalink":"/2d-Converter-Docs/docs/Sketch-A-Shape/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Sketch-A-Shape/introduction.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docSidebar","previous":{"title":"For Beginners","permalink":"/2d-Converter-Docs/docs/Sketch-A-Shape/beginners"},"next":{"title":"Stage I Auto-Encoder","permalink":"/2d-Converter-Docs/docs/Sketch-A-Shape/stage_1"}}');var a=r(4848),i=r(8453);const o={},s="Introduction",d={},c=[{value:"Zero Shot Transfer Learning",id:"zero-shot-transfer-learning",level:2},{value:"Overview of Approach",id:"overview-of-approach",level:2},{value:"Stage I - Discrete Auto Encoder",id:"stage-i---discrete-auto-encoder",level:3},{value:"Stage II - Prior Model",id:"stage-ii---prior-model",level:3},{value:"Inference - Shape Generation",id:"inference---shape-generation",level:3}];function h(e){const t={a:"a",br:"br",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",p:"p",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"introduction",children:"Introduction"})}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.a,{href:"https://arxiv.org/abs/2307.03869",children:"[arXiv]"}),"\r\n",(0,a.jsx)(t.a,{href:"https://arxiv.org/pdf/2307.03869",children:"[pdf]"}),(0,a.jsx)(t.br,{}),"\n","This paper recognizes that there is a use for large pre-trained models in text-to-shape generation, and applies this\r\nlogic to sketch-to-shape generation as a way to circumvent the limited amount of paired sketch/shape datasets in\r\nexistence."]}),"\n",(0,a.jsx)(t.p,{children:"This approach demonstrates the effectiveness of a large pre-trained model to pick up on the semantic features of a\r\nsketch drawing allowing for shape generation without the paired sketch/shape dataset."}),"\n",(0,a.jsx)(t.h2,{id:"zero-shot-transfer-learning",children:"Zero Shot Transfer Learning"}),"\n",(0,a.jsx)(t.p,{children:"Zero Shot Learning leverages large pre-trained 2D-image/text models in downstream 3d vision tasks as seen with\r\ntext-to-3d shape generation. This paper is noted as the first to try to leverage this capability for 2d image-to-3d\r\nshape generation."}),"\n",(0,a.jsx)(t.h2,{id:"overview-of-approach",children:"Overview of Approach"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"Training Process Overview",src:r(5018).A+"",width:"2148",height:"1062"})}),"\n",(0,a.jsx)(t.h3,{id:"stage-i---discrete-auto-encoder",children:"Stage I - Discrete Auto Encoder"}),"\n",(0,a.jsx)(t.p,{children:"Initially, we start out by training an auto-encoder to compress and decompress 3d shapes. The research paper adopts the\r\nVQVAE structure for this model."}),"\n",(0,a.jsx)(t.h3,{id:"stage-ii---prior-model",children:"Stage II - Prior Model"}),"\n",(0,a.jsx)(t.p,{children:"For this step, images are fed into the pre-trained model and tailor fit into the compressed shape embeddings of the\r\nauto-encoder trained in Stage I. The research paper implements this with a combination of mapping network, self\r\nattention heads and a bi-directional transformer approach."}),"\n",(0,a.jsx)(t.h3,{id:"inference---shape-generation",children:"Inference - Shape Generation"}),"\n",(0,a.jsx)(t.p,{children:"During inference, we feed sketches into the large pre-trained model. The semantic features picked by by the large\r\npre-trained model are processed by the model trained in Stage II. This provides us with the compressed shape embeddings\r\nconsistent with the auto-encoder trained in Stage I, from which we can use the decoder to obtain the final 3d shape\r\nrendering."})]})}function p(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},5018:(e,t,r)=>{r.d(t,{A:()=>n});const n=r.p+"assets/images/approach-83de5dc4ac0e3d1c2d977362715220fa.png"},8453:(e,t,r)=>{r.d(t,{R:()=>o,x:()=>s});var n=r(6540);const a={},i=n.createContext(a);function o(e){const t=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),n.createElement(i.Provider,{value:t},e.children)}}}]);